# -*- coding: utf-8 -*-
"""0717주요변수찾기.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-qiKfCmNsdu4ElLhsHHgcSl8Sh2Y8oV
"""

!pip install --upgrade joblib==1.1.0

!pip install mglearn
import mglearn

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import seaborn as sns           # Seaborn 로딩하기
import matplotlib.pyplot as plt # Matplotlib의 pyplot 로딩하기

"""## 첫번째 분석"""

travel = pd.read_csv("/content/drive/MyDrive/2023문화관광공모전/2022외래관광객조사.csv",encoding="UTF-8")
display(travel)

display(travel.info())

travel.columns

df = travel[['Q7_01','Q3','Q3B1','Q4B1','RQ2A2','D_MON_season','D_NAT_GROUP', 'D_SEX']]
df

df = df.replace(" ",0)
travel = travel.replace(" ",0)

df = df.fillna(0)
travel = travel.fillna(0)

df2 = df[(df.Q3 == 1) | (df.Q3 == 2)]

travel = travel[(travel.Q3 == 1) | (travel.Q3 == 2)]

# 산점도 행렬 생성
sns.pairplot(df2)
plt.show()
# 고정 산도와 밀도, 고정 산도와 구연산은  양의 상관 관계, 고정 산도와 pH는 음의 상관관계를 가지고 있음을 확인.
sns.pairplot(df2)
plt.savefig('scatter_matrix.png')  # 그래프를 이미지 파일로 저장
plt.show()

#비선형 ->

"""# 0722

"""

#상관관계

correlation_matrix = df2.corr()

# 시각화
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='BuPu')
plt.title('Correlation Matrix')
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

X = ff[['Q3B1', 'RQ2A2', 'D_MON_season', 'D_SEX','D_AGE','Q4','Q4A01','Q4A02','Q4A03','Q4A04','Q4A05','Q4A06','Q4A07','Q4A08','Q4A09','Q4A10','Q4A11','Q4A12','Q4A13','Q4A14','Q4A15','Q4A16','Q4A17']]
y = ff['Q7_01']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)

tree = DecisionTreeClassifier(max_depth=4,random_state=0)
tree.fit(X_train, y_train)
print("학습용 데이터 정확도: {:.3f}".format(tree.score(X_train, y_train)))
print("시험용 데이터 정확도: {:.3f}".format(tree.score(X_test, y_test)))

from sklearn.tree import export_graphviz
import graphviz

export_graphviz(tree, out_file="tree.dot", class_names=[ "불만족", "만족"],
                feature_names=df2.columns[1:], impurity=False, filled=True)

with open("tree.dot") as f:
    dot_graph = f.read()

display(graphviz.Source(dot_graph))

#랜덤포레스트

from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators = 10, random_state =0)
forest.fit(X_train,y_train)

print("학습용 데이터 정확도: {:.3f}".format(forest.score(X_train, y_train)))
print("시험용 데이터 정확도: {:.3f}".format(forest.score(X_test, y_test)))

import numpy as np

def plot_feature_importances_df2(model):
    plt.rcParams["figure.figsize"] = (9,6)
    n_features = len(df2.columns[1:])  # 30개
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), df2.columns[1:])
    plt.xlabel("feature importance")
    plt.ylabel("feature")
    plt.ylim(-1, n_features) #Y 축 범위 지정

plot_feature_importances_df2(tree)

import numpy as np

def plot_feature_importances_df2(model):
    plt.rcParams["figure.figsize"] = (9,6)
    n_features = len(df2.columns[1:])  # 30개
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), df2.columns[1:])
    plt.xlabel("feature importance")
    plt.ylabel("feature")
    plt.ylim(-1, n_features) #Y 축 범위 지정

plot_feature_importances_df2(forest)

"""랜덤포레스트가 의사결정나무보다 과적합 문제를 완화시킬 수 있어서 선택!"""

#의사결정나무,랜덤포레스트 비교
from sklearn.model_selection import cross_val_score

# 교차 검증을 10번 수행하여 10번의 교차 검증 평균 정확도를 비교 (10-fold cross validation)
# default cv=5

dt_scores = cross_val_score(tree, X_train, y_train, cv=10, scoring='accuracy')
rf_scores = cross_val_score(forest, X_train, y_train, cv=10, scoring='accuracy')

print("Accuracy")
print("Decision tree: ", dt_scores)
print("Random forest: ", rf_scores)

print("Accuracy mean")
print("Decision tree :{:.3f}".format(dt_scores.mean()))
print("Random forest :{:.3f}".format(rf_scores.mean()))

cv_list = [
            ['decision_tree',dt_scores],
            ['random_forest',rf_scores],
          ]
df = pd.DataFrame.from_dict(dict(cv_list))
df.plot()

"""# 0722

"""

travel

display(travel.info())

"""## 전체데이터로 변경"""

travel = pd.read_csv("/content/drive/MyDrive/2023문화관광공모전/0722Data.csv",encoding="UTF-8")
display(travel)

travel = travel.replace(" ",0)
travel = travel.fillna(0)
travel = travel[(travel.Q3 == 1) | (travel.Q3 == 2)]
display(travel)

from pandas.core.internals.construction import default_index
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from matplotlib import pyplot as plt


# 데이터 분할
X = travel[['Q3', 'Q3B1', 'Q4B1', 'RQ2A2', 'D_MON_season', 'D_SEX','총액1인TOT','D_AGE','RQ1','Q4','D_NAT_GROUP','D_PLA','D_TYP','KWON1','KWON2','KWON3','KWON4','KWON5','KWON6','KWON7','KWON8']]
y = travel['Q7_01']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

'''# 모델 학습: 의사결정나무
dt = DecisionTreeClassifier(min_samples_split=700, random_state=42)
dt.fit(X_train, y_train)'''

# 의사결정나무에서 가장 좋은 min_samples_split 값 찾기
param_grid = {'min_samples_split': range(2, 1000)}
dt_ex = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(dt_ex, param_grid, cv=10, scoring='accuracy') #cv는 5겹 교차검증
grid_search.fit(X_train, y_train)

# 가장 좋은 파라미터 출력
print("Best Parameter: {}".format(grid_search.best_params_))

# 가장 좋은 파라미터로 훈련된 트리 Estimator 가져오기
best_dt = grid_search.best_estimator_

# 모델 학습: 랜덤포레스트
rf = RandomForestClassifier(n_estimators=1000, random_state=42)
rf.fit(X_train, y_train)

# 변수 중요도
importances_dt = best_dt.feature_importances_
importances_rf = rf.feature_importances_



############정확도 확인####################
from sklearn.metrics import accuracy_score

# 의사결정나무 모델 예측 및 정확도 확인
dt_pred = best_dt.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
# 의사결정나무 모델의 학습 데이터에 대한 정확도 확인
dt_train_accuracy = best_dt.score(X_train, y_train)
print("Decision Tree Train Accuracy: ", dt_train_accuracy)
print('Decision Tree Accuracy: ', dt_accuracy)

# 랜덤포레스트 모델 예측 및 정확도 확인
rf_pred = rf.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
# 랜덤포레스트 모델의 학습 데이터에 대한 정확도 확인
rf_train_accuracy = rf.score(X_train, y_train)
print("Random Forest Train Accuracy: ", rf_train_accuracy)
print('Random Forest Accuracy: ', rf_accuracy)
################################################

# 변수 중요도 시각화 - Decision Tree
plt.figure(figsize=(12, 12))
indices_dt = np.argsort(importances_dt)
plt.title('Feature Importances - DT')
plt.barh(range(len(indices_dt)), importances_dt[indices_dt], color='b', align='center')
plt.yticks(range(len(indices_dt)), [X.columns[i] for i in indices_dt])
plt.xlabel('Relative Importance')
plt.show()

# 변수 중요도 시각화 - Random Forest
plt.figure(figsize=(12, 12))
indices_rf = np.argsort(importances_rf)
plt.title('Feature Importances -RF')
plt.barh(range(len(indices_rf)), importances_rf[indices_rf], color='b', align='center')
plt.yticks(range(len(indices_rf)), [X.columns[i] for i in indices_rf])
plt.xlabel('Relative Importance')
plt.show()

ff = pd.read_csv("/content/drive/MyDrive/2023문화관광공모전/(0722)2022외래관광객조사.csv",encoding="euc-kr")
display(ff)

ff= ff.replace(" ",0)
ff = ff.fillna(0)
display(ff)

X = travel[['Q3B1', 'RQ2A2', 'D_MON_season', 'D_SEX','D_AGE','Q4','Q4A01','Q4A02','Q4A03','Q4A04','Q4A05','Q4A06','Q4A07','Q4A08','Q4A09','Q4A10','Q4A11','Q4A12','Q4A13','Q4A14','Q4A15','Q4A16','Q4A17']]
y = travel['Q7_01']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


tree = DecisionTreeClassifier(max_depth=4,random_state=0)
tree.fit(X_train, y_train)
print("학습용 데이터 정확도: {:.3f}".format(tree.score(X_train, y_train)))
print("시험용 데이터 정확도: {:.3f}".format(tree.score(X_test, y_test)))

"""##최종

"""

from pandas.core.internals.construction import default_index
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from matplotlib import pyplot as plt

# 데이터 분할
X = travel[['Q3B1', 'RQ2A2', 'D_MON_season', 'D_SEX','D_AGE','Q4','Q4A01','Q4A02','Q4A03','Q4A04','Q4A05','Q4A06','Q4A07','Q4A08','Q4A09','Q4A10','Q4A11','Q4A12','Q4A13','Q4A14','Q4A15','Q4A16','Q4A17']]
y = travel['Q7_01']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 의사결정나무에서 가장 좋은 min_samples_split 값 찾기
param_grid = {'min_samples_split': range(2, 500)}
dt_ex = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(dt_ex, param_grid, cv=20, scoring='accuracy') #cv는 5겹 교차검증
grid_search.fit(X_train, y_train)

# 가장 좋은 파라미터 출력
print("Best Parameter: {}".format(grid_search.best_params_))

# 가장 좋은 파라미터로 훈련된 트리 Estimator 가져오기
best_dt = grid_search.best_estimator_

# 모델 학습: 랜덤포레스트
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 변수 중요도
importances_dt = best_dt.feature_importances_
importances_rf = rf.feature_importances_



############정확도 확인####################
from sklearn.metrics import accuracy_score

# 의사결정나무 모델 예측 및 정확도 확인
dt_pred = best_dt.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
# 의사결정나무 모델의 학습 데이터에 대한 정확도 확인
dt_train_accuracy = best_dt.score(X_train, y_train)
print("Decision Tree Train Accuracy:{:.3f} ", dt_train_accuracy)
print('Decision Tree Accuracy: {:.3f} ', dt_accuracy)

# 랜덤포레스트 모델 예측 및 정확도 확인
rf_pred = rf.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
# 랜덤포레스트 모델의 학습 데이터에 대한 정확도 확인
rf_train_accuracy = rf.score(X_train, y_train)
print("Random Forest Train Accuracy:{:.3f} ", rf_train_accuracy)
print('Random Forest Accuracy:{:.3f} ', rf_accuracy)
################################################

# 변수 중요도 시각화 - Decision Tree
plt.figure(figsize=(12, 12))
indices_dt = np.argsort(importances_dt)
plt.title('Feature Importances - DT')
plt.barh(range(len(indices_dt)), importances_dt[indices_dt], color='b', align='center')
plt.yticks(range(len(indices_dt)), [X.columns[i] for i in indices_dt])
plt.xlabel('Relative Importance')
plt.show()

# 변수 중요도 시각화 - Random Forest
plt.figure(figsize=(12, 12))
indices_rf = np.argsort(importances_rf)
plt.title('Feature Importances -RF')
plt.barh(range(len(indices_rf)), importances_rf[indices_rf], color='b', align='center')
plt.yticks(range(len(indices_rf)), [X.columns[i] for i in indices_rf])
plt.xlabel('Relative Importance')
plt.show()

from sklearn.metrics import accuracy_score

# 의사결정나무 모델 예측 및 정확도 확인
dt_pred = best_dt.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
# 의사결정나무 모델의 학습 데이터에 대한 정확도 확인
dt_train_accuracy = best_dt.score(X_train, y_train)
print("Decision Tree Train Accuracy:{:.3f} ".format(dt_train_accuracy))
print('Decision Tree Accuracy: {:.3f} '.format(dt_accuracy))

# 랜덤포레스트 모델 예측 및 정확도 확인
rf_pred = rf.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
# 랜덤포레스트 모델의 학습 데이터에 대한 정확도 확인
rf_train_accuracy = rf.score(X_train, y_train)
print("Random Forest Train Accuracy:{:.3f} ".format(rf_train_accuracy))
print('Random Forest Accuracy:{:.3f} '.format(rf_accuracy))

from sklearn.ensemble import GradientBoostingClassifier
# 기본값은 max_depth= 3, n_estimators=100, learning_rate = 0.1

gbrt = GradientBoostingClassifier(random_state=0)
gbrt.fit(X_train, y_train)

print("학습용 데이터 세트 정확도: {:.3f}".format(gbrt.score(X_train, y_train)))
print("시험용 데이터 세트 정확도: {:.3f}".format(gbrt.score(X_test, y_test)))

"""#0717

"""

X = df2.iloc[:,1:]
y = df2.iloc[:,0]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)

tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, y_train)
print("학습용 데이터 정확도: {:.3f}".format(tree.score(X_train, y_train)))
print("시험용 데이터 정확도: {:.3f}".format(tree.score(X_test, y_test)))

print("학습용 데이터 정확도: {:.3f}".format(tree.score(X_train, y_train)))
print("시험용 데이터 정확도: {:.3f}".format(tree.score(X_test, y_test)))

from sklearn.tree import export_graphviz
import graphviz

# filled = True: 색상의 진하기로 클래스를 나타냄, impurity: 불순도 계산값 보여주기 옵션
export_graphviz(tree, out_file="tree.dot", class_names=[ "만족", "불만족"],
                feature_names=df2.columns[1:], impurity=False, filled=True)

with open("tree.dot") as f:
    dot_graph = f.read()

display(graphviz.Source(dot_graph))

"""###주요변수확인

"""

import numpy as np

def plot_feature_importances_df2(model):
    plt.rcParams["figure.figsize"] = (9,6)
    n_features = len(df2.columns[1:])
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), df2.columns[1:])
    plt.xlabel("feature importance")
    plt.ylabel("feature")
    plt.ylim(-1, n_features) #Y 축 범위 지정 (-1 ~ 30)

plot_feature_importances_df2(tree)

"""### 두번째 분석(국가제외)

"""

df2 = travel[['Q7_01','Q3','Q3B1','Q4B1','RQ2A2','D_MON_season', 'D_SEX',
       'D_AGE']]
df2



df2 = df2.replace(" ",0)

df2 = df2.fillna(0)
df

df2 = df[(df.Q3 == 1) | (df.Q3 == 2)]
df2

df2['Q7_01'] == 0

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

X = df2.iloc[:,1:]
y = df2.iloc[:,0]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state= 6)

tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, y_train)
print("학습용 데이터 정확도: {:.3f}".format(tree.score(X_train, y_train)))
print("시험용 데이터 정확도: {:.3f}".format(tree.score(X_test, y_test)))

from sklearn.tree import export_graphviz
import graphviz

# filled = True: 색상의 진하기로 클래스를 나타냄, impurity: 불순도 계산값 보여주기 옵션
export_graphviz(tree, out_file="tree.dot", class_names=[ "만족", "불만족"],
                feature_names=df2.columns[1:], impurity=False, filled=True)

with open("tree.dot") as f:
    dot_graph = f.read()

display(graphviz.Source(dot_graph))

"""# 위에 노드 길이 줄이기"""



"""###주요변수확인

"""

X = df2.iloc[:,1:]
y = df2.iloc[:,0]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state= 6)

tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, y_train)
print("학습용 데이터 정확도: {:.3f}".format(tree.score(X_train, y_train)))
print("시험용 데이터 정확도: {:.3f}".format(tree.score(X_test, y_test)))

import numpy as np
#의사결정나무로
def plot_feature_importances_df2(model):
    plt.rcParams["figure.figsize"] = (9,6)
    n_features = len(df2.columns[1:])
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), df2.columns[1:])
    plt.xlabel("feature importance")
    plt.ylabel("feature")
    plt.ylim(-1, n_features) #Y 축 범위 지정 (-1 ~ 30)


plot_feature_importances_df2(tree)

from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators=100, random_state=2)
forest.fit(X_train, y_train)

print("학습용 데이터 세트 정확도: {:.3f}".format(forest.score(X_train, y_train)))
print("시험용 데이터 세트 정확도: {:.3f}".format(forest.score(X_test, y_test)))

plot_feature_importances_df2(forest)

from sklearn.ensemble import GradientBoostingClassifier
gbrt = GradientBoostingClassifier(random_state=3)
gbrt.fit(X_train, y_train)

print("학습용 데이터 세트 정확도: {:.3f}".format(gbrt.score(X_train, y_train)))
print("시험용 데이터 세트 정확도: {:.3f}".format(gbrt.score(X_test, y_test)))

plot_feature_importances_df2(gbrt)

"""###추가한거임"""

travel2 = pd.read_csv("/content/drive/MyDrive/2023문화관광공모전/2022외래관광객조사.csv",encoding="UTF-8")
display(travel2)

df3 = travel2[['Q7_02','Q3','Q3B1','Q4B1','Q2a2','총액1인TOT','D_MON_season','D_NAT_GROUP', 'D_SEX',
       'D_AGE']]
df3

df3 = df3.replace(" ",0)

df3 = df3.fillna(0)

df4 = df3[(df3.Q3 == 1) | (df.Q3 == 2)]
df4

X = df4.iloc[:,1:]
y = df4.iloc[:,0]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)

tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, y_train)
print("학습용 데이터 정확도: {:.3f}".format(tree.score(X_train, y_train)))
print("시험용 데이터 정확도: {:.3f}".format(tree.score(X_test, y_test)))

# filled = True: 색상의 진하기로 클래스를 나타냄, impurity: 불순도 계산값 보여주기 옵션
export_graphviz(tree, out_file="tree.dot", class_names=[ "만족", "불만족"],
                feature_names=df4.columns[1:], impurity=False, filled=True)

with open("tree.dot") as f:
    dot_graph = f.read()

display(graphviz.Source(dot_graph))

def plot_feature_importances_df4(model):
    plt.rcParams["figure.figsize"] = (9,6)
    n_features = len(df4.columns[1:])  # 30개
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), df4.columns[1:])   # 30개의 눈금을 표시하고 눈금 레이블을 변수 이름으로 지정
    plt.xlabel("feature importance")
    plt.ylabel("feature")
    plt.ylim(-1, n_features) #Y 축 범위 지정 (-1 ~ 30)

plot_feature_importances_df4(tree)

display(travel.info())

